{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c20353",
   "metadata": {},
   "source": [
    "# Econ570 Assigment2 Shuxian Mao 6221281532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "64ef48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import graphviz as gr\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "random.seed(570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "34988e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation\n",
    "## Define a function to generate variance-covariance matrix\n",
    "def fn_generate_cov(dim, *corr):\n",
    "    # dim: number of covariates\n",
    "    # corr: correlation for multivariate normal\n",
    "    acc  = []\n",
    "    for i in range(dim):\n",
    "        row = np.ones((1,dim)) * corr\n",
    "        row[0][i] = 1\n",
    "        acc.append(row)\n",
    "    return np.concatenate(acc,axis=0)\n",
    "\n",
    "## Define a function to generate correlated random variables\n",
    "def fn_generate_multnorm(nobs,corr,nvar):\n",
    "# nobs: number of observations \n",
    "# nvar: number of covariates generated\n",
    "\n",
    "    mu = np.zeros(nvar)\n",
    "    std = (np.abs(np.random.normal(loc = 1, scale = .5,size = (nvar,1))))**(1/2)\n",
    "    # generate random normal distribution\n",
    "    acc = []\n",
    "    for i in range(nvar):\n",
    "        acc.append(np.reshape(np.random.normal(mu[i],std[i],nobs),(nobs,-1)))\n",
    "\n",
    "    normvars = np.concatenate(acc,axis=1)\n",
    "\n",
    "    cov = fn_generate_cov(nvar, corr)\n",
    "    C = np.linalg.cholesky(cov)\n",
    "\n",
    "    Y = np.transpose(np.dot(C,np.transpose(normvars)))\n",
    "    return Y\n",
    "\n",
    "\n",
    "# Define a function to randomly separate the samples to treated and non treated\n",
    "def fn_randomize_treatment(N,p=0.5):\n",
    "    # N: Number of observations\n",
    "    # p: proportion of treated people in the sample\n",
    "\n",
    "    treated = random.sample(range(N), round(N*p))\n",
    "    return np.array([(1 if i in treated else 0) for i in range(N)]).reshape([N,1])\n",
    "\n",
    "\n",
    "#Define a function to calculate the bias, rmse and size of treatment parameter\n",
    "def fn_bias_rmse_size(theta0,thetahat,se_thetahat,cval = 1.96):\n",
    "    # theta0: true parameter value\n",
    "    # thetatahat: estimated parameter value\n",
    "    # se_thetahat: estimated standard error of thetahat\n",
    "    b = thetahat - theta0\n",
    "    bias = np.mean(b)\n",
    "    rmse = np.sqrt(np.mean(b**2))\n",
    "    tval = b/se_thetahat # paramhat/se_paramhat H0: b = 0\n",
    "    size = np.mean(1*(np.abs(tval)>cval))\n",
    "\n",
    "    return (bias,rmse,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a7546",
   "metadata": {},
   "source": [
    "# Setting1: randomly assigned treatment and some observed covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c20388",
   "metadata": {},
   "source": [
    "$Y_i = \\tau_i * T_i+\\beta_i * X_i + e_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "91e7fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to simulate data\n",
    "def fn_generate_data(tau,N,p,beta0,corr = 0.5,conf = True,cov = True,covx = True,conxf = True):\n",
    "    # tau: treatment effect parameter\n",
    "    # N: Number of observations\n",
    "    # p: Number of covariates\n",
    "    # beta0: the matrix of true parameters of the observed covariates\n",
    "    nvar = p+1 # Add 1 confounder\n",
    "\n",
    "    if conf == False: # conf: confounding factors\n",
    "        conf_mult = 0 # remove confounder from outcome\n",
    "    else:\n",
    "        conf_mult = 1\n",
    "\n",
    "    if cov == False:   # cov: covariates factors\n",
    "        cov_mult = 0 # remove observed covariates from outcome\n",
    "    else:\n",
    "        cov_mult = 1\n",
    "\n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    C = allX[:,0].reshape([N,1]) # confounder\n",
    "    X = allX[:,1:] # observed covariates\n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "\n",
    "    Yab = tau*T+cov_mult*X@beta0+conf_mult*0.6*C+err\n",
    "\n",
    "    if covx == False:  # covx: Indicating whether we control the covariates\n",
    "        X = np.zeros([N,1])\n",
    "\n",
    "    if conxf == False:    # conxf: Indicating whether we control the confoundings\n",
    "        C = np.zeros([N,1])\n",
    "\n",
    "    return Yab, T, X, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "aa768c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "tau = 2.5\n",
    "N = 100\n",
    "p = 1\n",
    "beta0 = np.array([[5]]).reshape(1,-1)\n",
    "\n",
    "Y, T, X, C = fn_generate_data(tau,N,p,beta0 ,corr = 0.5,conf = False,cov = True,covx = True,conxf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29c5e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = np.hstack([Y,T,X,C])\n",
    "data_1 = pd.DataFrame(data_1)\n",
    "data_1.columns = [\"Y\",\"Treatment\",\"X\",\"Confounder\"]\n",
    "data_1.to_csv(\"data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e738ee21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"134pt\" height=\"116pt\"\r\n",
       " viewBox=\"0.00 0.00 134.00 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 130,-112 130,4 -4,4\"/>\r\n",
       "<!-- T -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>T</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">T</text>\r\n",
       "</g>\r\n",
       "<!-- Y -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>Y</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Y</text>\r\n",
       "</g>\r\n",
       "<!-- T&#45;&gt;Y -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>T&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.3496,-72.7646C39.7115,-64.2831 45.1469,-53.7144 50.0413,-44.1974\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.2346,-45.6409 54.6957,-35.1473 47.0096,-42.4395 53.2346,-45.6409\"/>\r\n",
       "</g>\r\n",
       "<!-- X -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>X</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\r\n",
       "</g>\r\n",
       "<!-- X&#45;&gt;Y -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>X&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.6504,-72.7646C86.2885,-64.2831 80.8531,-53.7144 75.9587,-44.1974\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.9904,-42.4395 71.3043,-35.1473 72.7654,-45.6409 78.9904,-42.4395\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1de89eb49a0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DAG\n",
    "g = gr.Digraph()\n",
    "g.edge(\"T\", \"Y\")\n",
    "g.edge(\"X\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9a44c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to estimate the treatment effect of random sample using OLS\n",
    "def fn_estimate_params(Y,T,X,C):\n",
    "    # Y: Outcome value of Dependent Variable\n",
    "    # T: Indicating the treatment group 0/1\n",
    "    # X: observed covariates\n",
    "    # C: Confounders\n",
    "\n",
    "    covar = np.hstack([T,X,C])\n",
    "    idx = np.argwhere(np.all(covar[..., :] == 0, axis=0))\n",
    "    covars = np.delete(covar, idx, axis=1)      # remove columns with all zero value\n",
    "\n",
    "    mod = sm.OLS(Y,covars)\n",
    "    res = mod.fit()\n",
    "    tauhat = res.params[0]\n",
    "    se_tauhat = res.HC1_se[0]\n",
    "\n",
    "    return tauhat,se_tauhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8c71b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do the monte carlo simulation\n",
    "def run_mc_simulation(n_rep,tau,N,p,beta0,corr = 0.5,conf = False,cov = False,covx = True,conxf = True):\n",
    "\n",
    "    estDict = {}\n",
    "    for n in N:\n",
    "        tauhats = []\n",
    "        sehats = []\n",
    "        for rep in tqdm(range(n_rep)):\n",
    "            Y,T,X,C = fn_generate_data(tau,n,p,beta0,corr,conf,cov,covx,conxf)\n",
    "            tauhat,sehat = fn_estimate_params(Y,T,X,C)\n",
    "            tauhats = tauhats + [tauhat]\n",
    "            sehats = sehats + [sehat]\n",
    "        estDict[n] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "        }\n",
    "\n",
    "    return estDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8eb66615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to summarize the results of monte carlo simulation\n",
    "def summarize_mc_simulation(tau,n_rep,N,estDict):\n",
    "    tau0 = tau*np.ones([n_rep,1])\n",
    "    for N, results in estDict.items():\n",
    "        (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                             results['sehat'])\n",
    "\n",
    "        print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5d35d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1164.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:15<00:00, 130.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.009578454864767239, RMSE=0.7275087659435233, size=0.0545\n",
      "N=1000: bias=-0.0012293266285810943, RMSE=0.23631121363400598, size=0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#a: do not control any covariables\n",
    "np.random.seed(570)\n",
    "n_rep = 2000\n",
    "tau = 2.5\n",
    "N = [100,1000]\n",
    "p = 1\n",
    "beta0 = np.array([[5]]).reshape(1,-1)\n",
    "\n",
    "estDict = run_mc_simulation(n_rep,tau,N,p,beta0,corr = 0.5,conf = False,cov = True,covx = False,conxf = False)\n",
    "summarize_mc_simulation(tau,n_rep,N,estDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a11d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1048.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 412.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.0021216270970377494, RMSE=0.1274613139660042, size=0.0625\n",
      "N=1000: bias=-0.0012437360580361502, RMSE=0.0396492922737336, size=0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#b: Control for all covariable\n",
    "np.random.seed(570)\n",
    "n_rep = 2000\n",
    "tau = 2.5\n",
    "N = [100,1000]\n",
    "p = 1\n",
    "beta0 = np.array([[5]]).reshape(1,-1)\n",
    "\n",
    "estDict = run_mc_simulation(n_rep,tau,N,p,beta0,corr = 0.5,conf = False,cov = True,covx = True,conxf = False)\n",
    "summarize_mc_simulation(tau,n_rep,N,estDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f9924",
   "metadata": {},
   "source": [
    "Real-life eg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625357e",
   "metadata": {},
   "source": [
    "Consider a group of unemployed individuals, \n",
    "some of whom are given a policy intervention (treatment group) \n",
    "and the rest are given no treatment (control group).\n",
    "In order to, study the effect of this policy (intervention) on the length of unemployment period,control their education level(observed covariable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d410d",
   "metadata": {},
   "source": [
    "# Setting2: DGP with confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "efefca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to separate the samples to treated and non treated based on confounders\n",
    "def fn_confounders_treatment(C,N):\n",
    "    # C: Confounders\n",
    "    # N: Number of observations (Sample size)\n",
    "\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    treated = 1 + 2 * C + err\n",
    "    treated_prob = 1/(1+np.exp(-treated))\n",
    "\n",
    "    return np.array([(1 if treated_prob[i] >= 0.5 else 0) for i in range(N)]).reshape([N,1])\n",
    "\n",
    "#Define a function to simulate data\n",
    "def fn_generate_data(tau,N,p,beta0,corr = 0.5,conf = True,cov = True,covx = True,conxf = True):\n",
    "\n",
    "    nvar = p+1 # Add 1 confounder\n",
    "\n",
    "    if conf == False:\n",
    "        conf_mult = 0 # Remove confounder from outcome\n",
    "    else:\n",
    "        conf_mult = 1\n",
    "\n",
    "    if cov == False:\n",
    "        cov_mult = 0 # remove observed covariates from outcome\n",
    "    else:\n",
    "        cov_mult = 1\n",
    "\n",
    "    allX = fn_generate_multnorm(N,corr,nvar)\n",
    "    C = allX[:,0].reshape([N,1]) # confounder\n",
    "    X = allX[:,1:] # observed covariates\n",
    "\n",
    "    T = fn_confounders_treatment(C,N) # choose treated units with the effect of C\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "\n",
    "    Yab = tau*T+cov_mult*X@beta0+conf_mult*0.6*C+err\n",
    "\n",
    "    if covx == False:\n",
    "        X = np.zeros([N,1])\n",
    "\n",
    "    if conxf == False:\n",
    "        C = np.zeros([N,1])\n",
    "\n",
    "    return Yab, T, X, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b1ae8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "np.random.seed(570)\n",
    "tau = 2\n",
    "N = 100\n",
    "p = 1\n",
    "beta0 = np.array([[5]])  .reshape(1,-1)\n",
    "\n",
    "Y, T, X, C = fn_generate_data(tau,N,p,beta0,corr = 0.5,conf = True,cov = False,covx = False,conxf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6071fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = np.hstack([Y,T,X,C])\n",
    "data_2 = pd.DataFrame(data_2)\n",
    "data_2.columns = [\"Y\",\"Treatment\",\"X\",\"Confounder\"] \n",
    "data_2.to_csv(\"data_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b395ac36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"112pt\" height=\"188pt\"\r\n",
       " viewBox=\"0.00 0.00 112.05 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184 108.046,-184 108.046,4 -4,4\"/>\r\n",
       "<!-- Confunder -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>Confunder</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-162\" rx=\"50.0912\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Confunder</text>\r\n",
       "</g>\r\n",
       "<!-- T -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>T</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">T</text>\r\n",
       "</g>\r\n",
       "<!-- Confunder&#45;&gt;T -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>Confunder&#45;&gt;T</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.4641,-144.055C44.3797,-136.059 40.6276,-126.331 37.1827,-117.4\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.34,-115.859 33.4758,-107.789 33.809,-118.379 40.34,-115.859\"/>\r\n",
       "</g>\r\n",
       "<!-- Y -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>Y</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Y</text>\r\n",
       "</g>\r\n",
       "<!-- Confunder&#45;&gt;Y -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>Confunder&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.6538,-143.908C59.6758,-133.569 61.9808,-120.09 63,-108 64.3441,-92.0566 64.3441,-87.9434 63,-72 62.2834,-63.4991 60.9311,-54.3119 59.4884,-46.0122\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.9142,-45.2892 57.6538,-36.0925 56.0309,-46.5623 62.9142,-45.2892\"/>\r\n",
       "</g>\r\n",
       "<!-- T&#45;&gt;Y -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>T&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.3986,-72.411C36.5136,-64.3352 40.3337,-54.4312 43.8346,-45.3547\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.1265,-46.5458 47.4597,-35.9562 40.5955,-44.0267 47.1265,-46.5458\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1de89eb42b0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DAG\n",
    "g = gr.Digraph()\n",
    "g.edge(\"Confunder\", \"T\")\n",
    "g.edge(\"T\", \"Y\")\n",
    "g.edge(\"Confunder\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "70825cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1150.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:15<00:00, 130.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=0.005311506033693411, RMSE=0.1661674293376382, size=0.057\n",
      "N=1000: bias=0.0009153416436707818, RMSE=0.05191932522362733, size=0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## MC simulate\n",
    "# a: fail to control\n",
    "np.random.seed(20)\n",
    "n_rep = 2000\n",
    "tau = 2.5\n",
    "N = [100,1000]\n",
    "p = 1\n",
    "beta0 = np.array([[5]]).reshape(1,-1)\n",
    "\n",
    "estDict = run_mc_simulation(n_rep,tau,N,p,beta0,corr = 0.5,conf = True,cov = False,covx = False,conxf = False)\n",
    "summarize_mc_simulation(tau,n_rep,N,estDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "69e4d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1052.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 412.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.003007456623045294, RMSE=0.13491080499252348, size=0.0635\n",
      "N=1000: bias=-0.0009261386839264918, RMSE=0.04197211965869316, size=0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#b:do control the confounder\n",
    "np.random.seed(570)\n",
    "n_rep = 2000\n",
    "tau = 2.5\n",
    "N = [100,1000]\n",
    "p = 1\n",
    "beta0 = np.array([[5]]).reshape(1,-1)\n",
    "\n",
    "estDict = run_mc_simulation(n_rep,tau,N,p,beta0,corr = 0.5,conf = True,cov = False,covx = False,conxf = True)\n",
    "summarize_mc_simulation(tau,n_rep,N,estDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881346e4",
   "metadata": {},
   "source": [
    "Real-life example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94f730",
   "metadata": {},
   "source": [
    "Infection with neo-coronavirus increases mortality. Younger individuals have better immune systems and are therefore less likely to be infected with NIV and have a lower mortality rate; older individuals have poorer resistance and are at higher risk of infection, while older individuals have a higher mortality rate. Thus, \"age\" is a confounding factor in the relationship between \"neo-coronavirus infection\" and \"mortality\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32d73a",
   "metadata": {},
   "source": [
    "# Setting3: Selection bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0c8a642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to simulate data\n",
    "def fn_generate_selection_data(tau,N,sigma,beta,conts = False):\n",
    "    # tau: treatment effect parameter\n",
    "    # N: Number of observations\n",
    "    # conts: selection bias variable\n",
    "\n",
    "    T = fn_randomize_treatment(N) # choose treated units\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "    X = sigma*T + err\n",
    "    err = np.random.normal(0,1,[N,1])\n",
    "\n",
    "    Yab = tau*T+beta*X+err\n",
    "\n",
    "    if conts == False:\n",
    "        X = np.zeros([N,1])\n",
    "\n",
    "    return Yab, T, X\n",
    "\n",
    "# Define a function to estimate the treatment effect of random sample using OLS\n",
    "def fn_estimate_params(Y,T,X):\n",
    "    # Y: Outcome value of Dependent Variable\n",
    "    # T: Indicating the treatment group 0/1\n",
    "    # X: selection bias variable\n",
    "\n",
    "    covar = np.hstack([T,X])\n",
    "    idx = np.argwhere(np.all(covar[..., :] == 0, axis=0))\n",
    "    covars = np.delete(covar, idx, axis=1)\n",
    "\n",
    "    mod = sm.OLS(Y,covars)\n",
    "    res = mod.fit()\n",
    "    tauhat = res.params[0]\n",
    "    se_tauhat = res.HC1_se[0]\n",
    "\n",
    "    return tauhat,se_tauhat\n",
    "\n",
    "# Define a function to do the monte carlo simulation\n",
    "def run_mc_simulation(n_rep,tau,N,sigma,beta,conts = False):\n",
    "    # n_rep: Number of replication time for monte carlo simulation\n",
    "    # tau: treatment effect parameter\n",
    "    # N: Number of observations\n",
    "    # conts: selection bias variable\n",
    "\n",
    "    estDict = {}\n",
    "    for n in N:\n",
    "        tauhats = []\n",
    "        sehats = []\n",
    "        for rep in tqdm(range(n_rep)):\n",
    "            Y,T,X = fn_generate_selection_data(tau,n,sigma,beta,conts)\n",
    "            tauhat,sehat = fn_estimate_params(Y,T,X)\n",
    "            tauhats = tauhats + [tauhat]\n",
    "            sehats = sehats + [sehat]\n",
    "        estDict[n] = {\n",
    "        'tauhat':np.array(tauhats).reshape([len(tauhats),1]),\n",
    "        'sehat':np.array(sehats).reshape([len(sehats),1])\n",
    "        }\n",
    "\n",
    "    return estDict\n",
    "\n",
    "# Define a function to summarize the results of monte carlo simulation\n",
    "def summarize_mc_simulation(tau,sigma,beta,n_rep,N,estDict):\n",
    "    tau0 = (tau+beta* sigma)*np.ones([n_rep,1])\n",
    "    for N, results in estDict.items():\n",
    "        (bias,rmse,size) = fn_bias_rmse_size(tau0,results['tauhat'],\n",
    "                                             results['sehat'])\n",
    "\n",
    "        print(f'N={N}: bias={bias}, RMSE={rmse}, size={size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9433b1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"89pt\" height=\"188pt\"\r\n",
       " viewBox=\"0.00 0.00 89.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184 85,-184 85,4 -4,4\"/>\r\n",
       "<!-- T -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>T</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">T</text>\r\n",
       "</g>\r\n",
       "<!-- X -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>X</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\r\n",
       "</g>\r\n",
       "<!-- T&#45;&gt;X -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>T&#45;&gt;X</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.6014,-144.411C44.4864,-136.335 40.6663,-126.431 37.1654,-117.355\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.4045,-116.027 33.5403,-107.956 33.8735,-118.546 40.4045,-116.027\"/>\r\n",
       "</g>\r\n",
       "<!-- Y -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>Y</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Y</text>\r\n",
       "</g>\r\n",
       "<!-- T&#45;&gt;Y -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>T&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.6538,-143.908C59.6758,-133.569 61.9808,-120.09 63,-108 64.3441,-92.0566 64.3441,-87.9434 63,-72 62.2834,-63.4991 60.9311,-54.3119 59.4884,-46.0122\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.9142,-45.2892 57.6538,-36.0925 56.0309,-46.5623 62.9142,-45.2892\"/>\r\n",
       "</g>\r\n",
       "<!-- X&#45;&gt;Y -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>X&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.3986,-72.411C36.5136,-64.3352 40.3337,-54.4312 43.8346,-45.3547\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.1265,-46.5458 47.4597,-35.9562 40.5955,-44.0267 47.1265,-46.5458\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1de84d220d0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DAG\n",
    "g = gr.Digraph()\n",
    "g.edge(\"T\", \"X\")\n",
    "g.edge(\"T\", \"Y\")\n",
    "g.edge(\"X\", \"Y\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cba57769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "np.random.seed(570)\n",
    "tau = 2\n",
    "beta = 1.5\n",
    "sigma = 0.1\n",
    "N = 100\n",
    "\n",
    "tau1 = tau + beta* sigma #True tau\n",
    "\n",
    "Y, T, X = fn_generate_selection_data(tau,N,sigma,beta,conts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6e3685df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = np.hstack([Y,T,X])\n",
    "data_3 = pd.DataFrame(data_3)\n",
    "data_3.columns = [\"Y\",\"Treatment\",\"X\"]\n",
    "data_3.to_csv(\"data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cbd7d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1501.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:12<00:00, 158.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.1492841572377096, RMSE=0.20474954441113793, size=0.187\n",
      "N=1000: bias=-0.15085236839136712, RMSE=0.15729862114151177, size=0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#control for the variable in between the path from cause to effect\n",
    "np.random.seed(570)\n",
    "tau = 2\n",
    "beta = 1.5\n",
    "sigma = 0.1\n",
    "N = [100,1000]\n",
    "\n",
    "tau1 = tau + beta* sigma\n",
    "\n",
    "estDict = run_mc_simulation(n_rep,tau,N,sigma,beta,conts=True)\n",
    "summarize_mc_simulation(tau,sigma,beta,n_rep,N,estDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "225f0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1673.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:11<00:00, 169.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100: bias=-0.0012611827008381176, RMSE=0.25089972445772857, size=0.0485\n",
      "N=1000: bias=-0.00046392983084960393, RMSE=0.07913287774572977, size=0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Do not control for the variable in between the path from cause to effect\n",
    "np.random.seed(570)\n",
    "tau = 2\n",
    "beta = 1.5\n",
    "sigma = 0.1\n",
    "N = [100,1000]\n",
    "\n",
    "tau1 = tau + beta* sigma\n",
    "\n",
    "estDict = run_mc_simulation(n_rep,tau,N,sigma,beta,conts=False)\n",
    "summarize_mc_simulation(tau,sigma,beta,n_rep,N,estDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9743613",
   "metadata": {},
   "source": [
    "Real-life example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc40df",
   "metadata": {},
   "source": [
    "For the effect of education (T) on personal income (Y), while personal ability (X) tends to affect both educational choice (T) and personal income (Y), i.e., personal ability (X) is a confounder of educational attainment (T) and personal income (Y)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
